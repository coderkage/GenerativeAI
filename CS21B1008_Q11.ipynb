{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deepp\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\dense.py:85: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: sigmoid\n",
      "Training Time: 8.99 seconds\n",
      "Test Accuracy: 0.9676\n",
      "---\n",
      "Activation: relu\n",
      "Training Time: 8.45 seconds\n",
      "Test Accuracy: 0.9761\n",
      "---\n",
      "Activation: leaky_relu\n",
      "Training Time: 7.75 seconds\n",
      "Test Accuracy: 0.9738\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import time\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 28 * 28).astype('float32') / 255\n",
    "x_test = x_test.reshape(-1, 28 * 28).astype('float32') / 255\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "def build_model(activation):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(128, input_shape=(28 * 28,), activation=activation))\n",
    "    model.add(layers.Dense(64, activation=activation))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "activations = ['sigmoid', 'relu', 'leaky_relu']\n",
    "results = {}\n",
    "\n",
    "for act in activations:\n",
    "    if act == 'leaky_relu':\n",
    "        model = build_model(tf.nn.leaky_relu)\n",
    "    else:\n",
    "        model = build_model(act)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    history = model.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.2, verbose=0)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "    results[act] = {\n",
    "        'training_time': training_time,\n",
    "        'test_accuracy': test_acc,\n",
    "        'history': history.history\n",
    "    }\n",
    "\n",
    "for act, res in results.items():\n",
    "    print(f\"Activation: {act}\")\n",
    "    print(f\"Training Time: {res['training_time']:.2f} seconds\")\n",
    "    print(f\"Test Accuracy: {res['test_accuracy']:.4f}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing/Exploding Gradients: Sigmoid is more prone to vanishing gradients, especially in deeper networks, while ReLU and Leaky ReLU mitigate this issue. Sigmoid has slower training rate and lower accuracy compared to ReLU and Leaky ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
